{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c34059d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/dftortosa/Windows/Users/dftor/Documents/diego_docs/industry/data_incubator/capstone_project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the working directory\n",
    "import os\n",
    "os.chdir(\"/media/dftortosa/Windows/Users/dftor/Documents/diego_docs/industry/data_incubator/capstone_project/\")\n",
    "\n",
    "#check\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d866034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER can be used from vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#open instance\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c817dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters\n",
    "    using simple regex statements.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()).replace(\"#\", \"\")\n",
    "\n",
    "##CHECK REGULAR EXPRESSIONS\n",
    "#https://www.sciencedirect.com/science/article/pii/S104244312030072X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38c6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to be applied per row\n",
    "def sentiment_per_row(raw_tweet, sentiment_type=\"compound\"):\n",
    "        \n",
    "    #clean\n",
    "    tweet_cleaned = clean_tweet(raw_tweet)\n",
    "        #import preprocessor as p; p.clean()\n",
    "    \n",
    "    #get the negative sentiment\n",
    "    sentiment = analyzer.polarity_scores(tweet_cleaned)[sentiment_type]\n",
    "\n",
    "    #return\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9aeba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###CHECK THIS!!!\n",
    "\n",
    "#define a function to do a given operation grouping by day but considering the a previous date\n",
    "def groupby_calcs(operation, metric_name, args=\"\", n_previous_days=2, currency=\"\"):\n",
    "        \n",
    "    #get the string with the operation name\n",
    "    string_evaluate = 'sentiment_results_df' + currency + '.groupby(\"date\")[\"sentiment\"].' + operation + '(' + args + ').to_frame()'\n",
    "        #do operations for the sentiment of each day and then convert to df\n",
    "\n",
    "    #evaluate to get the DF with the calculations per day\n",
    "    data_frame = eval(string_evaluate)\n",
    "    \n",
    "    #extract the range of days for which we have data\n",
    "    time_range = pd.date_range(np.min(np.unique(data_frame.index)), \n",
    "                        np.max(np.unique(data_frame.index)))\n",
    "    \n",
    "    #fill with NaN those dates with data\n",
    "    data_frame = data_frame.reindex(time_range, fill_value=np.NaN)\n",
    "        #https://stackoverflow.com/questions/15610805/accessing-row-from-previous-day-in-pandas-dataframe-with-apply\n",
    "    \n",
    "    #get sentiment of the previous days\n",
    "    data_frame['sentiment'] = data_frame['sentiment'].shift(n_previous_days)\n",
    "        #https://stackoverflow.com/questions/19324453/add-missing-dates-to-pandas-dataframe\n",
    "        \n",
    "    #reset the index\n",
    "    data_frame = data_frame.reset_index(level=0)\n",
    "        #set the index (date) as column \n",
    "  \n",
    "    #change the column name for sentiment\n",
    "    data_frame = data_frame.rename(columns={'index': 'date', 'sentiment': metric_name + '_sent' + currency + '_previous_' + str(n_previous_days) + '_days'})\n",
    "\n",
    "    #append the df to the list\n",
    "    return data_frame\n",
    "    \n",
    "\n",
    "def groupby_calcs_across_dates(previous_date, currency=\"\"):\n",
    "    \n",
    "    #define a list of operations to do\n",
    "    metrics_sentiment = [\"quantile_0.05\",\n",
    "                         \"quantile_0.1\",\n",
    "                         \"quantile_0.2\",\n",
    "                         \"quantile_0.3\",\n",
    "                         \"quantile_0.4\",\n",
    "                         \"quantile_0.5\",\n",
    "                         \"quantile_0.6\",\n",
    "                         \"quantile_0.7\",\n",
    "                         \"quantile_0.8\",\n",
    "                         \"quantile_0.9\",\n",
    "                         \"quantile_0.95\",\n",
    "                         \"mean\",\n",
    "                         \"min\",\n",
    "                         \"max\",\n",
    "                         \"sum\",\n",
    "                         \"count\", \n",
    "                             #this can be useful to control for averages with low number of tweets\n",
    "                             #but also to consider how much people is talking about europes economy \n",
    "                         \"range\", \n",
    "                         \"var\", \n",
    "                         \"std\", \n",
    "                         \"iqr\"]\n",
    "    \n",
    "    #open list to save each calculation\n",
    "    list_data_frames = []\n",
    "    \n",
    "    #apply each operation\n",
    "    for metric in metrics_sentiment:\n",
    "        \n",
    "        #for std\n",
    "        if metric == \"std\":\n",
    "            #use ddof=0 to get zero for cases with 1 tweet per day (see above)\n",
    "            result_calc = groupby_calcs(operation=metric, \n",
    "                          metric_name=metric, \n",
    "                          args='ddof=0', \n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency)\n",
    "        elif \"quantile\" in metric: #if quantile\n",
    "            #get the operation name and the quantile number\n",
    "            operation_name = metric.split(\"_\")[0]\n",
    "            quantile_number = metric.split(\"_\")[1]\n",
    "            result_calc = groupby_calcs(operation=operation_name, \n",
    "                          metric_name=metric, \n",
    "                          args=quantile_number, \n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency) \n",
    "        elif \"range\" in metric:\n",
    "            result_calc = groupby_calcs(operation=\"apply\", \n",
    "                          metric_name=metric, \n",
    "                          args=\"lambda x: np.max(x) - np.min(x)\",\n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency)\n",
    "        elif \"iqr\" in metric:\n",
    "            result_calc = groupby_calcs(operation=\"apply\", \n",
    "                          metric_name=metric, \n",
    "                          args=\"lambda x: (np.percentile(x, 75) - np.percentile(x, 25))\",\n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency)   \n",
    "        else: \n",
    "            result_calc = groupby_calcs(operation=metric, \n",
    "                          metric_name=metric, \n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency)\n",
    "           \n",
    "        list_data_frames.append(result_calc)\n",
    "        \n",
    "    return list_data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tweets_df_usd = pd.read_json(\"data/json_files/hashtag_fed_twitter_1999_1_1.json\", lines=True)\n",
    "\n",
    "#reset the indeces to avoid problems when indexing\n",
    "tweets_df_usd = tweets_df_usd.reset_index(drop=True)\n",
    "    #use drop drop parameter to avoid the old index being added as a column\n",
    "\n",
    "#check for duplicated IDs\n",
    "tweets_df_usd[\"id\"].duplicated().any()\n",
    "\n",
    "#take a look to one of the duplicated IDs\n",
    "#example_duplicated_id = tweets_df_usd[tweets_df_usd[\"id\"].duplicated()].iloc[2,:].loc[\"id\"] #get the ID\n",
    "\n",
    "#get the URL for all tweets having the example ID and check if there are identical URLs\n",
    "#tweets_df.loc[tweets_df[\"id\"] == example_duplicated_id, \"url\"].duplicated().any()\n",
    "    #tweets with the same ID have the same URL, so they are the same tweet\n",
    "    \n",
    "#remove tweets with a duplicated id\n",
    "tweets_df_usd = tweets_df_usd.drop_duplicates(subset=['id'])\n",
    "\n",
    "#select only tweets in english\n",
    "tweets_df_usd_en = tweets_df_usd[tweets_df_usd[\"lang\"] == \"en\"]\n",
    "print(tweets_df_usd_en.shape)\n",
    "    #CHECK IF THE SENTIMENT MODEL USED CAN DEAL WITH OTHER LANGUAGES\n",
    "    \n",
    "    \n",
    "del(tweets_df_usd)\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "with mp.Pool(10) as pool:\n",
    "    tweets_df_usd_en[\"sentiment\"] = pool.map(sentiment_per_row, tweets_df_usd_en[\"rawContent\"])\n",
    "        #for each row in tweets_df_usd_en[\"rawContent\"], apply sentiment_per_row across 10 cores\n",
    "        #https://stackoverflow.com/questions/45545110/make-pandas-dataframe-apply-use-all-cores\n",
    "    pool.close()\n",
    "    \n",
    "#CHECK: https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\n",
    "\n",
    "#change the sign so positive sentiment is negative for EUR\n",
    "tweets_df_usd_en[\"sentiment\"] = -tweets_df_usd_en[\"sentiment\"]\n",
    "\n",
    "\n",
    "#get the date and sentiment\n",
    "sentiment_results_df_usd = tweets_df_usd_en[[\"date\", \"sentiment\"]]\n",
    "sentiment_results_df_usd\n",
    "\n",
    "del(tweets_df_usd_en)\n",
    "\n",
    "#leave only year, month and day\n",
    "sentiment_results_df_usd[\"date\"] = pd.to_datetime(sentiment_results_df_usd[\"date\"].dt.strftime('%Y-%m-%d'))\n",
    "    #https://stackoverflow.com/questions/38067704/how-to-change-the-datetime-format-in-pandas\n",
    "    #https://stackoverflow.com/questions/20689288/converting-pandas-columns-to-datetime64-including-missing-values\n",
    "sentiment_results_df_usd\n",
    "\n",
    "#CHECK WARNING\n",
    "\n",
    "#check we have rows ordered in chronologicla order\n",
    "sentiment_results_df_usd.equals(sentiment_results_df_usd.sort_index())\n",
    "\n",
    "#get sentiment 1 and 2 weeks ago, along with 1, 2, 3, 4 and 5 months ago\n",
    "list_dates = np.arange(1, 16, 1)\n",
    "#list_dates = [7, 14, 30, 60, 90, 120, 150, 180, 210, 240, 270]\n",
    "\n",
    "#open a pool with 10 processes\n",
    "pool_dates = mp.Pool(processes=10)\n",
    "\n",
    "#apply the function for each previous day in parallel across 10 cores\n",
    "list_data_frames_usd = [pool_dates.apply_async(groupby_calcs_across_dates, args=(previous_date, \"_usd\")).get() for previous_date in list_dates]\n",
    "    #https://stackoverflow.com/questions/42843203/how-to-get-the-result-of-multiprocessing-pool-apply-async\n",
    "\n",
    "#close the pool\n",
    "pool_dates.close()\n",
    "\n",
    "#flatten list\n",
    "flat_list_usd = [item for sublist in list_data_frames_usd for item in sublist]\n",
    "flat_list_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##merge all data.frames\n",
    "from functools import reduce\n",
    "\n",
    "#merge them using reduce\n",
    "df_merged = reduce(lambda left, right: pd.merge(left, right, on=['date'], how='inner'), [element for element in flat_list_usd])\n",
    "    #leave only those rows having date in the datasets of currency value and sentiment\n",
    "    #https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes\n",
    "df_merged\n",
    "    #note that here the previous eur pricing can be different than the eur pricing of the previous row\n",
    "    #because when merging we lose days without tweet sentiment, so there is no longer continuous days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb58a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove these NA cases\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the final data.frame with all metrics and eur pricing\n",
    "df_merged.to_csv(\"results/twitter_sentiment_data_usd.csv.gz\", compression=\"gzip\", index=False)\n",
    "#df_merged = pd.read_csv(\"results/twitter_sentiment_data.csv.gz\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
