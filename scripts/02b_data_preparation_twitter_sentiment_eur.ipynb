{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efc53a2",
   "metadata": {},
   "source": [
    "# Processing Twitter sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251dbaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/dftortosa/Windows/Users/dftor/Documents/diego_docs/industry/data_incubator/capstone_project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the working directory\n",
    "import os\n",
    "os.chdir(\"/media/dftortosa/Windows/Users/dftor/Documents/diego_docs/industry/data_incubator/capstone_project/\")\n",
    "\n",
    "#check\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92b5e9",
   "metadata": {},
   "source": [
    "## Loading tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c6f4c",
   "metadata": {},
   "source": [
    "We include tweets about the BCE, which is the entity in charge of protecting the value of the euro, but also tweets about debt crisis in Europe, because this can negatively impact the expectations around the euro. Also tweets about the economy of Europe, because this can influence the expectations of economic agents around the Euro. This includes the economy of all EU countries using the Euro. \n",
    "\n",
    "We can consider a fiat currency as a financial asset instead of a real asset. Therefore, it is the financial liability or debt that the state acquires with the holder of the fiat currency. The value of this financial asset depends on the expected services that this asset can give in the future. What can expect a Euro holder from the Euro? \n",
    "\n",
    "A clear service is the liquidity, this fiat currency has a stable value, thus economic agent value this because it can use it in the future with a similar value, so the agent holds this currency. But there is more that gives value to the fiat currency. The state (central bank) has assets to defend the value of the currency. If there is an excess of the currency in relation with the demand, the state can sell assets and take bak its currency from the market, decreasing its offer and hence stabilizing its value. The central bank usually acumulate assets that buy by selling its own currency, but there other assets probably more important, tax assets. When the state gets taxes, its reabsorbing its currency. If the goverment is in superavit, it is gaining more of its own currency that selling, thus an economic agent can expect that the value of that currency is going to be stable, becuase the state is not in need to increase a lot the offer of its currency in order to support its activities. If the demand of the currency decreases, the state has margin to reabsorv its currency. If the state has a big deficit, then the only way to get financial support is to print more moneay, decreasing a lot the value of the curreny. An economic agent does not want to buy this currency. If a state is in default, its currency will lose a lot of value. Similarly, if the central bank is buying low-quality assets like bonds of countries with high debts, then the potential ability of the central bank to control the value of the currency in the future will be disminished, because its portfolio is not strong enough to reabsorv its currency when its demand decreases. Therefore, it is useful to consider both the situation (balance) of the BCE and the situation of the countries with Euro. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Specially relevant those countries with sovereing debt problems. The BCE not only acts to preserve the value of the euro but also to avoid the bankruptcy of the members states, and both goals can be incompatible in certain circumstances (e.g., buy EU debt when at the same time you have to fight against inflation raising the interest rates). It is worth to mention that the Euros can be seen as liabilities that it has respect to those having euros. Of course, today we do not have gold standard, so we cannot go to the ECB ansd ask for an exchange in gold. However, the ECB has a mandate to control the value of the Euro so those with Euros will not lose value, because of this, there is a goal of 2% inflation per year, that is the limit. For example, if for eany reason there is an increase of inflation (e.g., due to high energy prices), the ECB should use its assets to buy Euros and hence remove them from circulation, this would decrease the offer of Euros, and hence increase the value of the currency. But what happens if the ECB has not enough assets to compensate? or it has assets of low quality like for example bonds from European countries with debt problems? The ECB could have difficulties to gain capitalization through these assets and use it to stabilize Euro's value. The economic agents know this because they can see the balance of the ECB, and they can decide to go away from Euro. This sentiment of the economic agents for the Euro is what I am trying to catch here, using twitter for that.\n",
    "\n",
    "We are also going to include tweets about ECB'presidents as a way to collect better the setiment around this institution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fdcfb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_paths = [\"search_euro_bank_twitter_1999_1_1.json\",\n",
    "              \"hashtag_euro_bank_twitter_1999_1_1.json\",\n",
    "              \"search_european_union_economy_twitter_1999_1_1.json\",\n",
    "              \"search_eu_debt_crisis_twitter_1999_1_1.json\", \n",
    "              \"search_greece_economy_twitter_1999_1_1.json\",\n",
    "              \"search_italy_economy_twitter_1999_1_1.json\",\n",
    "              \"search_spain_economy_twitter_1999_1_1.json\",\n",
    "              \"search_portugal_economy_twitter_1999_1_1.json\",\n",
    "              \"search_cyprus_economy_twitter_1999_1_1.json\",\n",
    "              \"search_slovenia_economy_twitter_1999_1_1.json\",\n",
    "              \"search_france_economy_twitter_1999_1_1.json\",\n",
    "              \"search_belgium_economy_twitter_1999_1_1.json\",\n",
    "              \"search_croatia_economy_twitter_1999_1_1.json\", \n",
    "              \"search_ireland_economy_twitter_1999_1_1.json\",\n",
    "              \"search_germany_economy_twitter_1999_1_1.json\", \n",
    "              \"search_netherlands_economy_twitter_1999_1_1.json\", \n",
    "              \"search_finland_economy_twitter_1999_1_1.json\", \n",
    "              \"search_austria_economy_twitter_1999_1_1.json\", \n",
    "              \"search_luxembourg_economy_twitter_1999_1_1.json\", \n",
    "              \"search_slovakia_economy_twitter_1999_1_1.json\", \n",
    "              \"search_malta_economy_twitter_1999_1_1.json\", \n",
    "              \"search_estonia_economy_twitter_1999_1_1.json\", \n",
    "              \"search_latvia_economy_twitter_1999_1_1.json\", \n",
    "              \"search_lithuania_economy_twitter_1999_1_1.json\", \n",
    "              \"search_trichet_twitter.json\", \n",
    "              \"hashtag_draghi_twitter.json\", \n",
    "              \"hashtag_lagarde_twitter.json\"]\n",
    "len(list_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76671f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "def read_tweet_data(path):\n",
    "    return pd.read_json(\"data/json_files/\"+path, lines=True)\n",
    "\n",
    "with mp.Pool(10) as pool:\n",
    "    list_input_df = list(pool.map(read_tweet_data, list_paths))\n",
    "        #for each row in tweets_df_en[\"rawContent\"], apply sentiment_per_row across 10 cores\n",
    "        #https://stackoverflow.com/questions/45545110/make-pandas-dataframe-apply-use-all-cores\n",
    "    pool.close()\n",
    "list_input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error \n",
    "    #alternative approach to load the json files, slower and larger memory footprint, so avoid running it\n",
    "%%time\n",
    "import pandas as pd\n",
    "list_input_df = list(map(lambda path: pd.read_json(\"data/json_files/\"+path, lines=True), list_paths))\n",
    "list_input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede086d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.concat(list_input_df)\n",
    "tweets_df[[\"id\", \"date\", \"rawContent\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acfce7",
   "metadata": {},
   "source": [
    "We are not using tweets about the USD because it is possible that the demand of US dollars increases (e.g., due to fears of recession and the corresponding search for safe heavens as it still has this reputation), but at the same time people can be unsatified with the way that the FED deals with inflation. In that scenario, the sentiment around USD in twitter would be negative in general, but it its demand can still increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39045d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process json file\n",
    "    #https://www.kaggle.com/code/prathamsharma123/clean-raw-json-tweets-data/notebook\n",
    "print(tweets_df.shape)\n",
    "    #as many rows as tweets\n",
    "    #as many columns as features in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33180df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see features\n",
    "print(tweets_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505b6d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get all columns for the first tweet\n",
    "print(tweets_df.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2203f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#reset the indeces to avoid problems when indexing\n",
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "    #use drop drop parameter to avoid the old index being added as a column\n",
    "tweets_df[[\"id\", \"date\", \"rawContent\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cda9c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check we have the expected number of rows\n",
    "tweets_df.shape[0] == sum([len(data_frame) for data_frame in list_input_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release space\n",
    "del(list_input_df)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398692dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicated IDs\n",
    "tweets_df[\"id\"].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look to one of the duplicated IDs\n",
    "example_duplicated_id = tweets_df[tweets_df[\"id\"].duplicated()].iloc[2,:].loc[\"id\"] #get the ID\n",
    "\n",
    "#get the URL for all tweets having the example ID and check if there are identical URLs\n",
    "tweets_df.loc[tweets_df[\"id\"] == example_duplicated_id, \"url\"].duplicated().any()\n",
    "    #tweets with the same ID have the same URL, so they are the same tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3c29d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove tweets with a duplicated id\n",
    "tweets_df = tweets_df.drop_duplicates(subset=['id'])\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e602a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check languages\n",
    "import numpy as np\n",
    "np.unique(tweets_df[\"lang\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53814a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see tweets not in english\n",
    "sum(tweets_df[\"lang\"] != \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da5db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select only tweets in english\n",
    "tweets_df_en = tweets_df[tweets_df[\"lang\"] == \"en\"]\n",
    "tweets_df_en.shape\n",
    "    #CHECK IF THE SENTIMENT MODEL USED CAN DEAL WITH OTHER LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release space\n",
    "del(tweets_df)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638ddd7",
   "metadata": {},
   "source": [
    "It seems that \"rawContent\" shows the raw tweet content, while \"renderedContent\" show the strings that would bet shown by the web app ([link](https://github.com/JustAnotherArchivist/snscrape/issues/479)).\n",
    "\n",
    "We are going to use the raw tweet because it seem it could have more information. They do not seem to be very different anyways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_en.loc[:, \"rawContent\"].iloc[0:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea485e49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_df_en.loc[:, \"renderedContent\"].iloc[0:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ff6a",
   "metadata": {},
   "source": [
    "## Sentiment analysis of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38edadeb",
   "metadata": {},
   "source": [
    "We will use VADER to perform the sentiment analysis. There are alternatives like TextBlob, but VADER meets well our requeriments\n",
    "- It is sensitive to both polarity (positive/negative) and intensity (strength) of emotion.\n",
    "- you get a positive, negative neutral scores and a compound score \n",
    "- labelled data not required because it is pre-trained ([link](https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664), [link](https://towardsdatascience.com/sentiment-analysis-of-tweets-167d040f0583))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take example tweet\n",
    "example_tweet = tweets_df_en.loc[1, \"rawContent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41bdbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER can be used from vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#open instance\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#example\n",
    "print(analyzer.polarity_scores(example_tweet))\n",
    "print(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097be155",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#or from nlTK\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#instance\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929de767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we get similar results\n",
    "print(sid.polarity_scores(example_tweet))\n",
    "print(analyzer.polarity_scores(example_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29795f9",
   "metadata": {},
   "source": [
    "Define function to clean the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac40189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters\n",
    "    using simple regex statements.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()).replace(\"#\", \"\")\n",
    "\n",
    "##CHECK REGULAR EXPRESSIONS\n",
    "#https://www.sciencedirect.com/science/article/pii/S104244312030072X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb325c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_tweet)\n",
    "clean_tweet(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da85186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analyzer.polarity_scores(clean_tweet(example_tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb8c59",
   "metadata": {},
   "source": [
    "We are going to use the compound sentiment. In this way, we consider both the negative and positive sentiment around the BCE and the economy of Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to be applied per row\n",
    "def sentiment_per_row(raw_tweet, sentiment_type=\"compound\"):\n",
    "        \n",
    "    #clean\n",
    "    tweet_cleaned = clean_tweet(raw_tweet)\n",
    "        #import preprocessor as p; p.clean()\n",
    "    \n",
    "    #get the negative sentiment\n",
    "    sentiment = analyzer.polarity_scores(tweet_cleaned)[sentiment_type]\n",
    "\n",
    "    #return\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e14791",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "with mp.Pool(10) as pool:\n",
    "    tweets_df_en[\"sentiment\"] = pool.map(sentiment_per_row, tweets_df_en[\"rawContent\"])\n",
    "        #for each row in tweets_df_en[\"rawContent\"], apply sentiment_per_row across 10 cores\n",
    "        #https://stackoverflow.com/questions/45545110/make-pandas-dataframe-apply-use-all-cores\n",
    "    pool.close()\n",
    "    \n",
    "#CHECK: https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc545d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the date and sentiment\n",
    "sentiment_results_df_eur = tweets_df_en[[\"date\", \"sentiment\"]]\n",
    "sentiment_results_df_eur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077879b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release space\n",
    "del(tweets_df_en)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e1f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#leave only year, month and day\n",
    "sentiment_results_df_eur[\"date\"] = pd.to_datetime(sentiment_results_df_eur[\"date\"].dt.strftime('%Y-%m-%d'))\n",
    "    #https://stackoverflow.com/questions/38067704/how-to-change-the-datetime-format-in-pandas\n",
    "    #https://stackoverflow.com/questions/20689288/converting-pandas-columns-to-datetime64-including-missing-values\n",
    "sentiment_results_df_eur\n",
    "\n",
    "#CHECK WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check we have rows ordered in chronologicla order\n",
    "sentiment_results_df_eur.equals(sentiment_results_df_eur.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08c050",
   "metadata": {},
   "source": [
    "Now let's calculate several metrics for the sentiment of each day and save them in list of data.frames. We can calculate the average, quartiles, std..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15050794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e70c0",
   "metadata": {},
   "source": [
    "We can use lambda to define our own calculations like the interquartile range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e362b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].apply(lambda a: (np.percentile(a, 75) - np.percentile(a, 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f56be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the most frequent value\n",
    "from statistics import mode\n",
    "mode([1,2,3,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe56846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#std gives NA for cases with n=1\n",
    "print(pd.DataFrame([1,1]).std()) #std = 0\n",
    "print(pd.DataFrame([1,2]).std()) #std = 0.7\n",
    "print(pd.DataFrame([1]).std()) #std = NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab4514",
   "metadata": {},
   "source": [
    "This is solved using ddof=0. ddof is Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame([1,1]).std(ddof=0)) #std = 0\n",
    "print(pd.DataFrame([1,2]).std(ddof=0)) #std = 0.7\n",
    "print(pd.DataFrame([1]).std(ddof=0)) #std = NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a164b1",
   "metadata": {},
   "source": [
    "We get the same results with np.std and .std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98f280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.std([1,1])) #std = 0\n",
    "print(np.std([1,2])) #std = 0.7\n",
    "print(np.std([1])) #std = NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e5783",
   "metadata": {},
   "source": [
    "We can calculate the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ae08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].apply(lambda x: np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f4782",
   "metadata": {},
   "source": [
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af17048",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1c716",
   "metadata": {},
   "source": [
    "We are going to use evaluate inside the function in order to perform the different operations we need. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326ede2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "operation=\"mean\"\n",
    "string_evaluate = 'sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].' + operation + '().to_frame()'\n",
    "string_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eff4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(string_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef2e14",
   "metadata": {},
   "source": [
    "We are interested in have a row per day, so we can exactly calculate the sentiment of 1, 10... days ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fbcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use min and max to select the earliest and latest dates\n",
    "print(np.min(np.unique(sentiment_results_df_eur[\"date\"])))\n",
    "print(np.max(np.unique(sentiment_results_df_eur[\"date\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd1c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first calculate the time range from the earliest to latest date\n",
    "idx = pd.date_range(np.min(np.unique(sentiment_results_df_eur[\"date\"])), \n",
    "                    np.max(np.unique(sentiment_results_df_eur[\"date\"])))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477901b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate the mean sentiment in tweets per day and then re-index considering all days between the earliest and \n",
    "#latest date. Those dates without tweets will have NaN\n",
    "sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].mean().reindex(idx, fill_value=np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44dfd7e",
   "metadata": {},
   "source": [
    "You can see how there is a gap without tweets between 11 and 15 of January and, accordingly, the days between have NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sentiment_results_df_eur[\"date\"])[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4763845",
   "metadata": {},
   "source": [
    "We can also shift the rows after making an operation per day, i.e., move the value of day 1 to day 2, value of day 2 to day 3, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b5de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first calculate the average sentiment per day\n",
    "sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].mean().reindex(idx, fill_value=np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38727d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now shift 2 days, so 2022-07-16 has now the average of the previous day (2022-07-15)\n",
    "sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].mean().reindex(idx, fill_value=np.NaN).shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee17910",
   "metadata": {},
   "source": [
    "Shift zero would be equal to the original variable, no change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c74cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].mean().reindex(idx, fill_value=np.NaN).shift(0).equals(sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].mean().reindex(idx, fill_value=np.NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88185834",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_results_df_eur[\"sentiment\"].shift(0).equals(sentiment_results_df_eur[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb0766",
   "metadata": {},
   "source": [
    "Now, we are going to calculate different summary statistics of the sentiment of the previous days (1, 5, 10, 60 days ago):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency = \"_usd\"\n",
    "'sentiment_results_df_eur' + currency + '.groupby(\"date\")[\"sentiment\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb219cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(sentiment_results_df_eur.groupby(\"date\")[\"sentiment\"].sum().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.unique(sentiment_results_df_eur[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653803e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###CHECK THIS!!!\n",
    "\n",
    "#define a function to do a given operation grouping by day but considering the a previous date\n",
    "def groupby_calcs(operation, metric_name, args=\"\", n_previous_days=2, currency=\"\"):\n",
    "        \n",
    "    #get the string with the operation name\n",
    "    string_evaluate = 'sentiment_results_df_eur' + currency + '.groupby(\"date\")[\"sentiment\"].' + operation + '(' + args + ').to_frame()'\n",
    "        #do operations for the sentiment of each day and then convert to df\n",
    "\n",
    "    #evaluate to get the DF with the calculations per day\n",
    "    data_frame = eval(string_evaluate)\n",
    "    \n",
    "    #extract the range of days for which we have data\n",
    "    time_range = pd.date_range(np.min(np.unique(data_frame.index)), \n",
    "                        np.max(np.unique(data_frame.index)))\n",
    "    \n",
    "    #fill with NaN those dates with data\n",
    "    data_frame = data_frame.reindex(time_range, fill_value=np.NaN)\n",
    "        #https://stackoverflow.com/questions/15610805/accessing-row-from-previous-day-in-pandas-dataframe-with-apply\n",
    "    \n",
    "    #get sentiment of the previous days\n",
    "    data_frame['sentiment'] = data_frame['sentiment'].shift(n_previous_days)\n",
    "        #https://stackoverflow.com/questions/19324453/add-missing-dates-to-pandas-dataframe\n",
    "        \n",
    "    #reset the index\n",
    "    data_frame = data_frame.reset_index(level=0)\n",
    "        #set the index (date) as column \n",
    "  \n",
    "    #change the column name for sentiment\n",
    "    data_frame = data_frame.rename(columns={'index': 'date', 'sentiment': metric_name + '_sent' + currency + '_previous_' + str(n_previous_days) + '_days'})\n",
    "\n",
    "    #append the df to the list\n",
    "    return data_frame\n",
    "    \n",
    "\n",
    "def groupby_calcs_across_dates(previous_date, currency=\"\"):\n",
    "    \n",
    "    #define a list of operations to do\n",
    "    metrics_sentiment = [\"quantile_0.05\",\n",
    "                         \"quantile_0.1\",\n",
    "                         \"quantile_0.2\",\n",
    "                         \"quantile_0.3\",\n",
    "                         \"quantile_0.4\",\n",
    "                         \"quantile_0.5\",\n",
    "                         \"quantile_0.6\",\n",
    "                         \"quantile_0.7\",\n",
    "                         \"quantile_0.8\",\n",
    "                         \"quantile_0.9\",\n",
    "                         \"quantile_0.95\",\n",
    "                         \"mean\",\n",
    "                         \"min\",\n",
    "                         \"max\",\n",
    "                         \"sum\",\n",
    "                         \"count\", \n",
    "                             #this can be useful to control for averages with low number of tweets\n",
    "                             #but also to consider how much people is talking about europes economy \n",
    "                         \"range\", \n",
    "                         \"var\", \n",
    "                         \"std\", \n",
    "                         \"iqr\"]\n",
    "    \n",
    "    #open list to save each calculation\n",
    "    list_data_frames = []\n",
    "    \n",
    "    #apply each operation\n",
    "    for metric in metrics_sentiment:\n",
    "        \n",
    "        #for std\n",
    "        if metric == \"std\":\n",
    "            #use ddof=0 to get zero for cases with 1 tweet per day (see above)\n",
    "            result_calc = groupby_calcs(operation=metric, \n",
    "                          metric_name=metric, \n",
    "                          args='ddof=0', \n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency)\n",
    "        elif \"quantile\" in metric: #if quantile\n",
    "            #get the operation name and the quantile number\n",
    "            operation_name = metric.split(\"_\")[0]\n",
    "            quantile_number = metric.split(\"_\")[1]\n",
    "            result_calc = groupby_calcs(operation=operation_name, \n",
    "                          metric_name=metric, \n",
    "                          args=quantile_number, \n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency) \n",
    "        elif \"range\" in metric:\n",
    "            result_calc = groupby_calcs(operation=\"apply\", \n",
    "                          metric_name=metric, \n",
    "                          args=\"lambda x: np.max(x) - np.min(x)\",\n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency)\n",
    "        elif \"iqr\" in metric:\n",
    "            result_calc = groupby_calcs(operation=\"apply\", \n",
    "                          metric_name=metric, \n",
    "                          args=\"lambda x: (np.percentile(x, 75) - np.percentile(x, 25))\",\n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency)   \n",
    "        else: \n",
    "            result_calc = groupby_calcs(operation=metric, \n",
    "                          metric_name=metric, \n",
    "                          n_previous_days=previous_date, \n",
    "                          currency=currency)\n",
    "           \n",
    "        list_data_frames.append(result_calc)\n",
    "        \n",
    "    return list_data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe46639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groupby_calcs_across_dates(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60059c98",
   "metadata": {},
   "source": [
    "We are going to calculate the summary statistics for the sentiment several days before the current day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aadedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentiment 1 and 2 weeks ago, along with 1, 2, 3, 4 and 5 months ago\n",
    "list_dates = np.arange(1, 16, 1)\n",
    "#list_dates = [7, 14, 30, 60, 90, 120, 150, 180, 210, 240, 270]\n",
    "\n",
    "#open a pool with 10 processes\n",
    "pool_dates = mp.Pool(processes=10)\n",
    "\n",
    "#apply the function for each previous day in parallel across 10 cores\n",
    "list_data_frames = [pool_dates.apply_async(groupby_calcs_across_dates, args=(previous_date, \"_eur\")).get() for previous_date in list_dates]\n",
    "    #https://stackoverflow.com/questions/42843203/how-to-get-the-result-of-multiprocessing-pool-apply-async\n",
    "\n",
    "#close the pool\n",
    "pool_dates.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release space\n",
    "del(sentiment_results_df_eur)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check we have the correct number of dfs in the list\n",
    "#len(list_data_frames) == len(metrics_sentiment)*len(list_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d50ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list_data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e0aaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_data_frames[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b869b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_data_frames[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261b759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in list_data_frames for item in sublist]\n",
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4b6f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##merge all data.frames\n",
    "from functools import reduce\n",
    "\n",
    "#merge them using reduce\n",
    "df_merged = reduce(lambda left, right: pd.merge(left, right, on=['date'], how='inner'), [element for element in flat_list])\n",
    "    #leave only those rows having date in the datasets of currency value and sentiment\n",
    "    #https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes\n",
    "df_merged\n",
    "    #note that here the previous eur pricing can be different than the eur pricing of the previous row\n",
    "    #because when merging we lose days without tweet sentiment, so there is no longer continuous days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a3445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#see NaN\n",
    "print(df_merged.isnull().sum())\n",
    "    #We have NaNs for days that do not have prior twitter data in the selected previous day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can just set as zero those NaN cases because we are using a compound sentiment (-1 to 1)\n",
    "#0 is nuetral and lower (negative) values means negative sentiment\n",
    "df_merged[\"quantile_0.05_sent_eur_previous_1_days\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove these NA cases\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that all days with just 1 tweet have std=0\n",
    "#np.unique(df_merged[df_merged[\"count_sent\"] == 1][\"std_sent\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81036be1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "##plot the metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "#for each metric\n",
    "for metric in metrics_sentiment:\n",
    "    \n",
    "    #open a plot with two panels\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,4))\n",
    "\n",
    "    #plot scatter against date and eur pricing\n",
    "    ax1.scatter(x=df_merged[\"date\"], y=df_merged[metric + \"_sent\"], s=0.5)\n",
    "    ax2.scatter(x=df_merged[\"percent_change_pricing\"], y=df_merged[metric + \"_sent\"], s=0.5)\n",
    "    \n",
    "    #set ylabel\n",
    "    ax1.set_ylabel(metric + \" negative sentiment\")\n",
    "    ax1.set_xlabel(\"Year\")\n",
    "    ax2.set_ylabel(metric + \" negative sentiment\")\n",
    "    ax2.set_xlabel(\"Percent change in EUR/USD exchange rate\")\n",
    "    \n",
    "    #close\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebe9db",
   "metadata": {},
   "source": [
    "Mean shows that low currency values tend to be associated with high negative sentiment. As the value increase, there is more variation. With more negative but specially much more less negative days. This is even clearer for max. The main difference is that high currency values are associated with a lot of 0 negative days.\n",
    "\n",
    "It seems that once economic problems arise, then worries extend across economic agents and hence twitter, making less likely to have positive/neutral tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd216401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the final data.frame with all metrics and eur pricing\n",
    "df_merged.to_csv(\"results/twitter_sentiment_data_eur.csv.gz\", compression=\"gzip\", index=False)\n",
    "#df_merged = pd.read_csv(\"results/twitter_sentiment_data.csv.gz\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
